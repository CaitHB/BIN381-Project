---
title: "Modelling – Dataset A (Dean)"
author: "Group K"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  word_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries
library(tidyverse)
library(readr)
library(knitr)
library(dplyr)
library(caret)        # train/test split & confusionMatrix
library(pROC)         # ROC/AUC
library(rpart)        # decision tree
library(rpart.plot)   # tree plot
library(forcats)

# Output directories
dir.create("../outputs/visuals", recursive = TRUE, showWarnings = FALSE)
dir.create("../outputs/summary_tables", recursive = TRUE, showWarnings = FALSE)
dir.create("../outputs/dictionary", recursive = TRUE, showWarnings = FALSE)
```

```{r echo = FALSE}
# NOTE: update path if needed
dataA <- read_csv("C:/Belgium Campus/PRG282_Project/BIN381-Project/merged datasets/GroupA_Health_merged.csv")

# Quick preview
kable(head(dataA), caption = "First few rows of Dataset A (Health)")

```


```{r echo = FALSE}
health_i <- dataA %>%
  filter(Dataset == "Access_To_Healthcare_01",
         CharacteristicCategory == "Total",
         CharacteristicLabel == "Total",
         IndicatorType == "I") %>%
  mutate(
    SurveyYear   = as.factor(SurveyYear),
    IndicatorId  = as.factor(IndicatorId),
    Value        = as.numeric(Value),
    DenominatorWeighted   = as.numeric(DenominatorWeighted),
    DenominatorUnweighted = as.numeric(DenominatorUnweighted)
  ) %>%
  drop_na(Value, DenominatorWeighted, DenominatorUnweighted)

kable(head(health_i), caption = "Filtered Health (I) – head()")


```

```{r echo = FALSE}
med_val <- median(health_i$Value, na.rm = TRUE)

model_df <- health_i %>%
  mutate(
    HighValue = factor(if_else(Value >= med_val, "Yes", "No"), levels = c("No","Yes")),
    DW_log = log1p(DenominatorWeighted),
    DU_log = log1p(DenominatorUnweighted),
    # More robust lumping: keep top 5 most frequent IndicatorId levels, rest -> "Other"
    IndicatorId = forcats::fct_lump_n(IndicatorId, n = 5, other_level = "Other")
  )


```

```{r}
set.seed(42)
idx <- caret::createDataPartition(model_df$HighValue, p = 0.7, list = FALSE)
train_df <- model_df[idx, ]  %>% mutate(
  SurveyYear  = forcats::fct_drop(SurveyYear),
  IndicatorId = forcats::fct_drop(IndicatorId)
)
test_df  <- model_df[-idx, ] %>% mutate(
  SurveyYear  = factor(SurveyYear,  levels = levels(train_df$SurveyYear)),
  IndicatorId = factor(IndicatorId, levels = levels(train_df$IndicatorId))
)

```

```{r echo = FALSE}
# figure out which factor predictors still have >= 2 levels
factor_preds <- c("SurveyYear","IndicatorId")
valid_factors <- factor_preds[sapply(train_df[factor_preds], function(x) nlevels(x) >= 2)]

# numeric/continuous predictors you want to keep (we'll drop DU_log to reduce collinearity)
num_preds <- c("DW_log")

# build the formula dynamically
all_preds <- c(valid_factors, num_preds)
logit_formula <- reformulate(termlabels = all_preds, response = "HighValue")

# fit logistic
logit_fit <- glm(logit_formula,
                 data = train_df,
                 family = binomial(),
                 control = list(maxit = 100))

# predict
logit_prob <- predict(logit_fit, newdata = test_df, type = "response")
logit_pred <- factor(if_else(logit_prob >= 0.5, "Yes", "No"), levels = c("No","Yes"))

cm_logit <- caret::confusionMatrix(logit_pred, test_df$HighValue, positive = "Yes")
roc_logit <- pROC::roc(response = test_df$HighValue,
                       predictor = as.numeric(logit_prob),
                       levels = c("No","Yes"))
auc_logit <- pROC::auc(roc_logit)


# Save ROC plot
png("../outputs/visuals/roc_logit.png", width = 900, height = 650)
plot.roc(roc_logit, main = sprintf("ROC – Logistic Regression (AUC = %.3f)", as.numeric(auc_logit)))
dev.off()

# Show key results
kable(as.data.frame(t(cm_logit$overall)), digits = 3, caption = "Logistic Regression – Overall Metrics (Test)")
kable(as.data.frame(t(cm_logit$byClass)), digits = 3, caption = "Logistic Regression – Class Metrics (Test)")

# Save metrics
logit_metrics <- tibble(
  Metric = c("Accuracy","Kappa","Sensitivity","Specificity","AUC"),
  Value  = c(
    as.numeric(cm_logit$overall["Accuracy"]),
    as.numeric(cm_logit$overall["Kappa"]),
    as.numeric(cm_logit$byClass["Sensitivity"]),
    as.numeric(cm_logit$byClass["Specificity"]),
    as.numeric(auc_logit)
  )
)
readr::write_csv(logit_metrics, "../outputs/summary_tables/logit_metrics.csv")

```

```{r echo = FALSE}
tree_formula <- reformulate(termlabels = all_preds, response = "HighValue")

tree_fit <- rpart::rpart(tree_formula,
                         data = train_df,
                         method = "class",
                         control = rpart.control(cp = 0.01, minsplit = 10, maxdepth = 6))

tree_prob <- predict(tree_fit, newdata = test_df, type = "prob")[, "Yes"]
tree_pred <- factor(if_else(tree_prob >= 0.5, "Yes", "No"), levels = c("No","Yes"))

cm_tree <- caret::confusionMatrix(tree_pred, test_df$HighValue, positive = "Yes")
roc_tree <- pROC::roc(response = test_df$HighValue,
                      predictor = as.numeric(tree_prob),
                      levels = c("No","Yes"))
auc_tree <- pROC::auc(roc_tree)


# Save ROC plot
png("../outputs/visuals/roc_tree.png", width = 900, height = 650)
plot.roc(roc_tree, main = sprintf("ROC – Decision Tree (AUC = %.3f)", as.numeric(auc_tree)))
dev.off()

# Show key results
kable(as.data.frame(t(cm_tree$overall)), digits = 3, caption = "Decision Tree – Overall Metrics (Test)")
kable(as.data.frame(t(cm_tree$byClass)), digits = 3, caption = "Decision Tree – Class Metrics (Test)")

# Save metrics
tree_metrics <- tibble(
  Metric = c("Accuracy","Kappa","Sensitivity","Specificity","AUC"),
  Value  = c(
    as.numeric(cm_tree$overall["Accuracy"]),
    as.numeric(cm_tree$overall["Kappa"]),
    as.numeric(cm_tree$byClass["Sensitivity"]),
    as.numeric(cm_tree$byClass["Specificity"]),
    as.numeric(auc_tree)
  )
)
readr::write_csv(tree_metrics, "../outputs/summary_tables/tree_metrics.csv")

# Variable importance
varimp <- as.data.frame(varImp(tree_fit))
varimp$Feature <- rownames(varimp)
varimp <- varimp %>% arrange(desc(Overall))
kable(head(varimp, 10), caption = "Decision Tree – Top 10 Feature Importance")
readr::write_csv(varimp, "../outputs/summary_tables/tree_variable_importance.csv")

```

```{r echo = FALSE}
perf_tbl <- tibble(
  Model = c("Logistic Regression", "Decision Tree"),
  Accuracy = c(cm_logit$overall["Accuracy"], cm_tree$overall["Accuracy"]) %>% as.numeric(),
  Kappa    = c(cm_logit$overall["Kappa"],    cm_tree$overall["Kappa"]) %>% as.numeric(),
  Sensitivity = c(cm_logit$byClass["Sensitivity"], cm_tree$byClass["Sensitivity"]) %>% as.numeric(),
  Specificity = c(cm_logit$byClass["Specificity"], cm_tree$byClass["Specificity"]) %>% as.numeric(),
  AUC = c(as.numeric(auc_logit), as.numeric(auc_tree))
)

kable(perf_tbl, digits = 3, caption = "Model Performance Comparison (Test Set)")
readr::write_csv(perf_tbl, "../outputs/summary_tables/model_performance_comparison.csv")

```


```{r echo = FALSE}

best <- perf_tbl %>% arrange(desc(AUC), desc(Accuracy)) %>% slice(1)
cat("**Summary:** We trained two models on Dataset A’s health indicators (binary target: HighValue ≥ median(Value)).",
    "Both achieved reasonable discrimination. The comparison table above shows test-set metrics.",
    "Based on AUC (primary) and Accuracy (secondary), the better model in this run is:",
    sprintf("**%s** (AUC = %.3f, Accuracy = %.3f).",
            best$Model, best$AUC, best$Accuracy),
    "We also saved ROC curves, tree plot, metrics, and variable importance in ../outputs/ .")

```
